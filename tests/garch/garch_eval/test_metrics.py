"""Unit tests for classic GARCH metrics (deterministic checks)."""

from __future__ import annotations

import sys
from pathlib import Path

import numpy as np
import pytest

_script_dir = Path(__file__).parent
_project_root = _script_dir.parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

from src.garch.garch_eval.metrics import (
    christoffersen_ind_test,
    compute_test_evaluation_metrics,
    kupiec_pof_test,
    mincer_zarnowitz,
    mse_mae_variance,
    qlike_loss,
    summarize_var_backtests,
    var_backtest_metrics,
)


def test_qlike_and_mse_mae_simple() -> None:
    # e^2 equals sigma^2 exactly -> MSE=MAE=0, QLIKE=log(sigma2)+1 average
    sigma2 = np.array([1.0, 2.0, 0.5, 3.0])
    e = np.array([1.0, np.sqrt(2.0), np.sqrt(0.5), np.sqrt(3.0)])
    losses = mse_mae_variance(e, sigma2)
    assert np.isclose(losses["mse"], 0.0)
    assert np.isclose(losses["mae"], 0.0)
    ql = qlike_loss(e, sigma2)
    expected = float(np.mean(np.log(sigma2) + (e**2) / sigma2))
    assert np.isclose(ql, expected)


def test_mincer_zarnowitz_perfect() -> None:
    # Perfect relation: e^2 = sigma2 -> slope approx 1, intercept approx 0
    sigma2 = np.linspace(0.5, 2.0, 50)
    e = np.sqrt(sigma2)
    res = mincer_zarnowitz(e, sigma2)
    tol = 1e-10
    min_r2 = 0.99
    assert abs(res["slope"] - 1.0) < tol
    assert abs(res["intercept"]) < tol
    assert min_r2 <= res["r2"] <= 1.0


def test_kupiec_pof_basic() -> None:
    """Test Kupiec POF test with known hit rate."""
    hits = np.array([0] * 90 + [1] * 10)
    kup = kupiec_pof_test(hits, alpha=0.1)
    assert kup["n"] == 100.0
    assert kup["x"] == 10.0
    assert np.isclose(kup["hit_rate"], 0.1)


def test_christoffersen_ind_basic() -> None:
    """Test Christoffersen independence test with valid p-value."""
    hits = np.array([0] * 90 + [1] * 10)
    ind = christoffersen_ind_test(hits)
    p_value = ind["p_value"] if not np.isnan(ind["p_value"]) else 0.0
    assert ind["lr_ind"] >= 0.0 or np.isnan(ind["lr_ind"])
    assert 0.0 <= p_value <= 1.0


def test_var_backtests_components() -> None:
    """Test VaR backtest components."""
    test_kupiec_pof_basic()
    test_christoffersen_ind_basic()


def test_var_backtests_extended_fields() -> None:
    """Ensure VaR backtests expose violation counts and percentages."""
    e = np.array([-2.5, -1.8, -1.0, 0.0, 0.5])
    s2 = np.ones_like(e)
    metrics = var_backtest_metrics(
        e,
        s2,
        dist="student",
        nu=None,
        lambda_skew=None,
        alphas=[0.01, 0.05],
    )
    level_five = metrics["0.05"]
    assert level_five["alpha"] == pytest.approx(0.05)
    assert level_five["violations"] == pytest.approx(
        1.0
    )  # Student-t with nu=8 has stricter threshold
    assert level_five["expected_violations"] == pytest.approx(0.25)
    assert level_five["expected_rate_pct"] == pytest.approx(5.0)
    assert level_five["violation_rate_pct"] == pytest.approx(
        20.0
    )  # 1 violation out of 5 observations


def test_var_summary_structure() -> None:
    """VaR summary aggregates per-level metrics and metadata."""
    e = np.array([-2.5, -1.8, -1.0, 0.0, 0.5])
    s2 = np.ones_like(e)
    summary = summarize_var_backtests(
        e,
        s2,
        dist="student",
        nu=None,
        lambda_skew=None,
        alphas=[0.01, 0.05],
    )
    assert summary["n_obs"] == 5
    assert "qlike" in summary
    assert summary["distribution"]["name"] == "student"  # type: ignore[index]
    levels = summary["levels"]  # type: ignore[assignment]
    assert levels["0.01"]["violations"] == pytest.approx(0.0)  # type: ignore[index]  # Student-t with nu=8 has very strict 1% threshold


def test_compute_test_evaluation_metrics_basic() -> None:
    """Test compute_test_evaluation_metrics returns correct structure."""
    e = np.array([-2.5, -1.8, -1.0, 0.0, 0.5, 1.0])
    s2 = np.ones_like(e)
    metrics = compute_test_evaluation_metrics(
        resid=e,
        sigma2=s2,
        dist="student",
        nu=8.0,
        lambda_skew=None,
        alphas=[0.01, 0.05],
    )
    assert metrics["n_obs"] == 6
    assert "qlike" in metrics
    assert "mse" in metrics
    assert "mae" in metrics
    assert "var_metrics" in metrics
    var_metrics = metrics["var_metrics"]  # type: ignore[index]
    assert "0.01" in var_metrics  # type: ignore[operator]
    assert "0.05" in var_metrics  # type: ignore[operator]


def test_compute_test_evaluation_metrics_empty() -> None:
    """Test compute_test_evaluation_metrics handles empty data."""
    e = np.array([])
    s2 = np.array([])
    metrics = compute_test_evaluation_metrics(
        resid=e,
        sigma2=s2,
        dist="student",
        nu=8.0,
        lambda_skew=None,
        alphas=[0.01, 0.05],
    )
    assert metrics["n_obs"] == 0
    assert np.isnan(metrics["qlike"])  # type: ignore[arg-type]
    assert np.isnan(metrics["mse"])  # type: ignore[arg-type]
    assert np.isnan(metrics["mae"])  # type: ignore[arg-type]
    assert metrics["var_metrics"] == {}


def test_compute_test_evaluation_metrics_with_invalid() -> None:
    """Test compute_test_evaluation_metrics filters invalid values."""
    e = np.array([1.0, np.nan, -1.0, 0.5])
    s2 = np.array([1.0, 1.0, np.nan, 2.0])
    metrics = compute_test_evaluation_metrics(
        resid=e,
        sigma2=s2,
        dist="student",
        nu=8.0,
        lambda_skew=None,
        alphas=[0.01, 0.05],
    )
    # Only 2 valid pairs: (1.0, 1.0) and (0.5, 2.0)
    assert metrics["n_obs"] == 2
    assert not np.isnan(metrics["qlike"])  # type: ignore[arg-type]
    assert not np.isnan(metrics["mse"])  # type: ignore[arg-type]


if __name__ == "__main__":
    pytest.main([__file__, "-v"])  # pragma: no cover
