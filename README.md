# S&P 500 Forecasting: Impact des Features ARIMA-GARCH sur LightGBM

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## ğŸ“‹ Description

Ce projet Ã©value l'impact de l'ajout de features issues d'un modÃ¨le **ARIMA-GARCH** sur les performances de prÃ©vision d'un modÃ¨le **LightGBM** pour les rendements du S&P 500.

### Objectif Principal

Tester l'hypothÃ¨se selon laquelle les modÃ¨les ARIMA-GARCH capturent des patterns temporels et de volatilitÃ© conditionnelle qui peuvent enrichir un modÃ¨le LightGBM et amÃ©liorer ses prÃ©visions de rendements.

### Approche MÃ©thodologique

Le projet suit une architecture modulaire en pipelines sÃ©quentiels :

1. **Pipeline de DonnÃ©es** : Collecte, nettoyage et agrÃ©gation pondÃ©rÃ©e par liquiditÃ©
2. **Pipeline ARIMA** : ModÃ©lisation des rendements avec optimisation et Ã©valuation
3. **Pipeline GARCH** : ModÃ©lisation de la volatilitÃ© conditionnelle (EGARCH)
4. **Pipeline LightGBM** : ModÃ¨le ML avec features ARIMA-GARCH

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10 ou supÃ©rieur
- pip ou conda

### Installation des dÃ©pendances

```bash
# Cloner le repository
git clone <repository-url>
cd S&P500_Forecasting*

# CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les dÃ©pendances
pip install -e .

# Pour le dÃ©veloppement (optionnel)
pip install -e ".[dev]"
```

### DÃ©pendances principales

- `pandas` : Manipulation de donnÃ©es
- `numpy` : Calculs numÃ©riques
- `statsmodels` : ModÃ¨les ARIMA
- `scikit-learn` : LightGBM et mÃ©triques
- `optuna` : Optimisation bayÃ©sienne
- `shap` : InterprÃ©tabilitÃ© ML
- `yfinance` : TÃ©lÃ©chargement de donnÃ©es
- `matplotlib`, `seaborn` : Visualisations

## ğŸ“ Structure du Projet

```
S&P500_Forecasting*/
â”œâ”€â”€ data/                    # DonnÃ©es brutes et intermÃ©diaires
â”‚   â”œâ”€â”€ dataset.csv          # DonnÃ©es historiques S&P 500
â”‚   â”œâ”€â”€ weighted_log_returns.csv
â”‚   â””â”€â”€ lightgbm_dataset_*.csv     # Datasets pour LightGBM
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_fetching/       # Collecte des donnÃ©es
â”‚   â”œâ”€â”€ data_cleaning/       # Nettoyage et validation
â”‚   â”œâ”€â”€ data_conversion/     # AgrÃ©gation pondÃ©rÃ©e
â”‚   â”œâ”€â”€ data_preparation/    # Split train/test
â”‚   â”œâ”€â”€ arima/               # Pipeline ARIMA
â”‚   â”‚   â”œâ”€â”€ optimisation_arima/
â”‚   â”‚   â”œâ”€â”€ training_arima/
â”‚   â”‚   â””â”€â”€ evaluation_arima/
â”‚   â”œâ”€â”€ garch/               # Pipeline GARCH
â”‚   â”‚   â”œâ”€â”€ structure_garch/
â”‚   â”‚   â”œâ”€â”€ garch_params/
â”‚   â”‚   â”œâ”€â”€ training_garch/
â”‚   â”‚   â”œâ”€â”€ garch_diagnostic/
â”‚   â”‚   â”œâ”€â”€ garch_eval/
â”‚   â”‚   â””â”€â”€ rolling_garch/
â”‚   â”œâ”€â”€ lightgbm/        # Pipeline LightGBM
â”‚   â”‚   â”œâ”€â”€ data_preparation/
â”‚   â”‚   â”œâ”€â”€ optimisation/
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â””â”€â”€ ablation/
â”œâ”€â”€ results/                 # RÃ©sultats et modÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ models/              # ModÃ¨les sauvegardÃ©s
â”‚   â”œâ”€â”€ garch/               # RÃ©sultats GARCH
â”‚   â””â”€â”€ lightgbm/       # RÃ©sultats LightGBM
â”œâ”€â”€ plots/                   # Visualisations
â”œâ”€â”€ documentation/           # Documentation mÃ©thodologique
â”‚   â””â”€â”€ methodologie.txt     # Documentation dÃ©taillÃ©e
â”œâ”€â”€ requirements.txt         # DÃ©pendances principales
â”œâ”€â”€ requirements-dev.txt    # DÃ©pendances dÃ©veloppement
â””â”€â”€ README.md               # Ce fichier
```

## ğŸ¯ Utilisation

### ExÃ©cution du Pipeline Complet

Pour exÃ©cuter l'ensemble du pipeline de bout en bout :

```bash
python src/main_global.py
```

Ce script dÃ©couvre automatiquement tous les modules et les exÃ©cute dans le bon ordre :

1. Collecte et prÃ©paration des donnÃ©es
2. Optimisation et entraÃ®nement ARIMA
3. Estimation et entraÃ®nement GARCH
4. GÃ©nÃ©ration des features pour LightGBM
5. Optimisation et entraÃ®nement LightGBM
6. Ã‰valuation

### ExÃ©cution Module par Module

Vous pouvez Ã©galement exÃ©cuter chaque module individuellement :

```bash
# Pipeline de donnÃ©es
python src/data_fetching/main.py
python src/data_cleaning/main.py
python src/data_conversion/main.py
python src/data_preparation/main.py

# Pipeline ARIMA
python src/arima/optimisation_arima/main.py
python src/arima/training_arima/main.py
python src/arima/evaluation_arima/main.py

# Pipeline GARCH
python src/garch/structure_garch/main.py
python src/garch/garch_params/main.py
python src/garch/training_garch/main.py
python src/garch/garch_diagnostic/main.py
python src/garch/garch_eval/main.py
python src/garch/rolling_garch/main.py

# Pipeline LightGBM
python src/lightgbm/data_preparation/main.py
python src/lightgbm/optimisation/main.py
python src/lightgbm/training/main.py
python src/lightgbm/eval/main.py
python src/lightgbm/ablation/main.py
```

# Modes d'Ã©valuation GARCH (`src/garch/garch_eval/main.py`)

Le script d'Ã©valuation accepte le flag `--forecast-mode` afin de sÃ©lectionner la stratÃ©gie de
gÃ©nÃ©ration des prÃ©visions plein-Ã©chantillon (TRAIN + TEST) avant calcul des mÃ©triques et
gÃ©nÃ©ration du dataset LightGBM :

- `no_refit` *(valeur par dÃ©faut officielle)* : appelle
  `generate_full_sample_forecasts_from_trained_model`, rÃ©utilise les paramÃ¨tres entraÃ®nÃ©s sans
  aucun refit sur les splits TRAIN/TEST et garantit que les features exportÃ©es restent
  cohÃ©rentes pour LightGBM.
- `hybrid` : appelle `generate_full_sample_forecasts_hybrid`, ce qui rÃ©active le planning de
  refit optimisÃ© (Ã  utiliser uniquement en connaissance de cause, car cela rÃ©Ã©crit les
  features en mode refit).

Le mode sÃ©lectionnÃ© est loggÃ© en dÃ©but d'exÃ©cution ainsi qu'au moment de la gÃ©nÃ©ration du
fichier `data_tickers_full_insights`, ce qui permet d'auditer chaque run a posteriori.

### Tests

ExÃ©cuter les tests unitaires :

```bash
# Tous les tests
pytest

# Tests spÃ©cifiques
pytest src/arima/optimisation_arima/test_optimisation_arima.py
pytest src/lightgbm/training/test_training.py

# Avec couverture (si pytest-cov installÃ©)
pytest --cov=src
```

## ğŸ“Š RÃ©sultats Principaux

### DonnÃ©es

- **PÃ©riode** : 2013-01-01 Ã  2024-12-31 (12 ans)
- **Tickers** : ~500 composantes du S&P 500
- **Split** : 80% train / 20% test (temporel)

### ModÃ¨les

- **ARIMA** : Optimisation sur grille d'hyperparamÃ¨tres, sÃ©lection par AIC/BIC
- **EGARCH(1,1)** : Estimation pour distributions Normale, Student-t, Skew-t
- **LightGBM** : Optimisation bayÃ©sienne avec Optuna, walk-forward CV

### Features LightGBM

- **Features ARIMA-GARCH** :
  - `arima_pred_return` : PrÃ©visions ARIMA
  - `arima_residual_return` : RÃ©sidus ARIMA
  - `sigma2_garch` : Variance conditionnelle
  - `sigma_garch` : VolatilitÃ© conditionnelle
  - `std_resid_garch` : RÃ©sidus standardisÃ©s GARCH

- **Indicateurs techniques** :
  - RSI (14), SMA (20), EMA (20), MACD

- **Features laggÃ©es** : Windows [1, 5, 10, 20] jours

### MÃ©triques d'Ã‰valuation

- **ARIMA** : MSE, RMSE, MAE, MAPE, Ljung-Box test
- **GARCH** : MSE, MAE, QLIKE, MZ regression, VaR coverage
- **LightGBM** : MSE, RMSE, MAE, RÂ², Feature importances, SHAP

## ğŸ“ˆ Visualisations

Les visualisations sont gÃ©nÃ©rÃ©es automatiquement dans le dossier `plots/` :

- **ARIMA** : ACF/PACF, rÃ©sidus, prÃ©visions rolling
- **GARCH** : Structure ARCH, diagnostics, prÃ©visions de volatilitÃ©
- **LightGBM** : CorrÃ©lations, SHAP values, importance des features

## ğŸ“š Documentation

Pour une documentation mÃ©thodologique dÃ©taillÃ©e, consultez :

```
documentation/methodologie.txt
```

Cette documentation couvre :

- L'architecture complÃ¨te du projet
- La mÃ©thodologie de chaque pipeline
- Les formules et Ã©quations utilisÃ©es
- La structure des donnÃ©es
- Les hypothÃ¨ses testÃ©es

## ğŸ”¬ HypothÃ¨ses TestÃ©es

1. **H1** : Les features ARIMA-GARCH amÃ©liorent les prÃ©visions du LightGBM
   - TestÃ© via comparaison Complete vs Without Insights

2. **H2** : La variance conditionnelle (sigma2_garch) est une feature importante
   - TestÃ© via ablation study (Complete vs Without Sigma2)

3. **H3** : Le modÃ¨le ARIMA-GARCH capture des patterns de volatilitÃ© significatifs
   - TestÃ© via diagnostics GARCH (ARCH-LM, Ljung-Box)

## ğŸ”„ ReproductibilitÃ©

Le projet garantit la reproductibilitÃ© via :

- **Random state fixe** : 42 (DEFAULT_RANDOM_STATE)
- **PÃ©riode de donnÃ©es fixe** : 2013-2024 (pas de mise Ã  jour automatique)
- **Split temporel fixe** : 80/20
- **Seeds fixÃ©s** pour numpy, pandas, sklearn, optuna

## âš™ï¸ Configuration

Les constantes et paramÃ¨tres par dÃ©faut sont dÃ©finis dans :

```
src/constants.py
```

Principaux paramÃ¨tres configurables :

- PÃ©riode de donnÃ©es
- Split train/test ratio
- HyperparamÃ¨tres d'optimisation
- FenÃªtres de lag
- FrÃ©quence de refit (rolling forecasts)

### Grille d'optimisation EGARCH

La recherche Optuna sur les hyperparamÃ¨tres EGARCH parcourt explicitement la grille
finie suivante (Ã©galement loggÃ©e lors de l'exÃ©cution) :

- Ordres : o âˆˆ {1, 2}, p âˆˆ {1, 2}
- Distributions : {normal, student, skewt}
- FrÃ©quences de refit : {5, 10, 20, 30, 60} jours
- Types de fenÃªtre : {expanding, rolling}
- Tailles de fenÃªtre rolling : {500, 1000} observations

Cela reprÃ©sente 60 combinaisons pour les fenÃªtres expanding et 120 combinaisons
pour les fenÃªtres rolling, soit 180 essais possibles explorÃ©s par le RandomSampler
avec `GARCH_OPTIMIZATION_N_TRIALS`.

## ğŸ› DÃ©pannage

### Erreurs courantes

1. **ModuleNotFoundError** : VÃ©rifier que toutes les dÃ©pendances sont installÃ©es

   ```bash
   pip install -r requirements.txt
   ```

2. **Fichiers manquants** : ExÃ©cuter les pipelines dans l'ordre

   ```bash
   python src/main_global.py
   ```

3. **Erreurs de mÃ©moire** : RÃ©duire la taille des datasets ou optimiser les paramÃ¨tres

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Pour contribuer :

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

### Standards de code

- Suivre PEP 8
- Utiliser type hints
- Ajouter des docstrings (style Google/NumPy)
- Ã‰crire des tests unitaires
- VÃ©rifier avec `black` et `ruff` avant de commit

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¤ Auteur

Projet dÃ©veloppÃ© dans le cadre d'une Ã©tude sur l'impact des features ARIMA-GARCH sur les modÃ¨les de machine learning pour la prÃ©vision financiÃ¨re.

## ğŸ™ Remerciements

- BibliothÃ¨ques open-source utilisÃ©es : pandas, statsmodels, scikit-learn, optuna, shap
- DonnÃ©es : yfinance pour les donnÃ©es historiques du S&P 500

## ğŸ“§ Contact

Pour toute question ou suggestion, n'hÃ©sitez pas Ã  ouvrir une issue sur le repository.

---

**Note** : Ce projet est Ã  des fins Ã©ducatives et de recherche. Les rÃ©sultats ne constituent pas des conseils financiers.
